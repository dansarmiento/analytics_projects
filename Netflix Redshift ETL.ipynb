{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb13f63c-c261-4e5d-a7df-bb0c88ca4b5b",
   "metadata": {},
   "source": [
    "**This notebook is an example process to develop ETL to create a pipeline that will feed Tableau with daily updates for player retention**\n",
    "\n",
    "* Evaluate the data\n",
    "* Create and optimize query\n",
    "* Optimize table if I participate in the data engineering\n",
    "* Calculate player retention rates (can also do daily and monthly active users)\n",
    "* Save as parquet for data efficiency (considering millions of rows)\n",
    "* Create Tableau .hyper file and load into Tableau server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4637fa96-fd3f-4671-b9f0-7f7c1cc85d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First components would be completed in the AWS Redshift notebook environment\n",
    "# Using camp grove as I saw it being promoted on the landing page \n",
    "# Evaluate the table structure\n",
    "SELECT * \n",
    "FROM camp_grove_sessions \n",
    "LIMIT 10;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a7b8bd-9396-4ff4-8d92-940a6246a6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update statistics for query optimization\n",
    "ANALYZE camp_grove_sessions;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7f9ee3-24d8-4038-add3-cfe6d968bead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the execution plan before running the queyr\n",
    "# If the query performs a full table scan, there may be missing indexes or improper sorting\n",
    "EXPLAIN \n",
    "SELECT player_id, session_duration \n",
    "FROM camp_grove_sessions \n",
    "WHERE session_date >= CURRENT_DATE - INTERVAL '90 days';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7b7e54-35db-4601-941d-4ef77991a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improve query performance by setting a distribution key and sort key for popular columns\n",
    "# DISTKEY ensures even data distribution to improve join efficiency\n",
    "# SORTKEY optimizes queries filtered by date ranges\n",
    "ALTER TABLE camp_grove_sessions \n",
    "SET DISTSTYLE KEY \n",
    "DISTKEY (player_id) \n",
    "SORTKEY (session_date);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fc9639-afa5-4893-972a-6c2098526994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vacuum reorganizes table storage for optimal query performance\n",
    "VACUUM camp_grove_sessions;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596e20f8-4b36-427b-834e-887948d86b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate player retention rates \n",
    "CREATE TABLE retention_rates AS\n",
    "SELECT \n",
    "    install_date, \n",
    "    COUNT(DISTINCT player_id) AS new_players,\n",
    "    COUNT(DISTINCT CASE WHEN session_date = install_date + INTERVAL '1 day' THEN player_id END) AS day_1_retention,\n",
    "    COUNT(DISTINCT CASE WHEN session_date = install_date + INTERVAL '7 day' THEN player_id END) AS day_7_retention,\n",
    "    COUNT(DISTINCT CASE WHEN session_date = install_date + INTERVAL '30 day' THEN player_id END) AS day_30_retention\n",
    "FROM camp_grove_sessions\n",
    "GROUP BY install_date;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcadf31d-0ff7-4433-a665-594ec6e50749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export this data to parquet files and store on S3\n",
    "UNLOAD ('SELECT * FROM retention_rates')\n",
    "TO 's3://your-bucket/camp_grove/retention_'\n",
    "IAM_ROLE 'arn:aws:iam::your-account-id:role/your-redshift-role'\n",
    "FORMAT AS PARQUET;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be6e32-3dc7-4528-a21d-d898f59d6d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have AWS Glue, this would be a serverless process to create a Tableau .hyper file using the parquet file\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from tableauhyperapi import HyperProcess, Connection, Telemetry, TableDefinition, SqlType, Inserter\n",
    "from awsglue.context import GlueContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Glue and Spark\n",
    "spark = SparkSession.builder.appName(\"GlueToTableauHyper\").getOrCreate()\n",
    "glueContext = GlueContext(spark)\n",
    "\n",
    "# Load Parquet from S3\n",
    "s3_path = \"s3://your-bucket/camp_grove/retention_\"\n",
    "df_spark = spark.read.parquet(s3_path)\n",
    "df = df_spark.toPandas()  # Convert to Pandas\n",
    "\n",
    "# Define Hyper File Path\n",
    "hyper_file_path = \"/tmp/retention_data.hyper\"\n",
    "\n",
    "# Define Hyper Table Schema\n",
    "table_def = TableDefinition(\"retention_data\", [\n",
    "    (\"install_date\", SqlType.date()),\n",
    "    (\"new_players\", SqlType.int()),\n",
    "    (\"day_1_retention\", SqlType.int()),\n",
    "    (\"day_7_retention\", SqlType.int()),\n",
    "    (\"day_30_retention\", SqlType.int()),\n",
    "])\n",
    "\n",
    "# Convert to Hyper File\n",
    "with HyperProcess(telemetry=Telemetry.DO_NOT_SEND_USAGE_DATA) as hyper:\n",
    "    with Connection(endpoint=hyper.endpoint, database=hyper_file_path, create_mode=\"CREATE_AND_REPLACE\") as connection:\n",
    "        connection.catalog.create_table(table_def)\n",
    "        with Inserter(connection, table_def) as inserter:\n",
    "            inserter.add_rows(df.itertuples(index=False, name=None))\n",
    "            inserter.execute()\n",
    "\n",
    "print(f\"Hyper file created: {hyper_file_path}\")\n",
    "\n",
    "# Upload to S3\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.upload_file(hyper_file_path, \"your-bucket\", \"retention_data.hyper\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1619cca2-0539-4a29-99e2-397397560894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last component would be to send this file to the server to update the visualization\n",
    "import requests\n",
    "\n",
    "\n",
    "# AWS S3 Configuration\n",
    "S3_BUCKET_NAME = \"your-s3-bucket\"\n",
    "S3_OBJECT_NAME = \"retention_data.hyper\"\n",
    "LOCAL_HYPER_FILE = \"/tmp/retention_data.hyper\"  \n",
    "\n",
    "# Download `.hyper` from S3\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.download_file(S3_BUCKET_NAME, S3_OBJECT_NAME, LOCAL_HYPER_FILE)\n",
    "print(f\"Downloaded {S3_OBJECT_NAME} from S3 to {LOCAL_HYPER_FILE}\")\n",
    "\n",
    "# Tableau Server Configuration\n",
    "TABLEAU_SERVER_URL = \"https://tableau.netflix.com\"  # Or whatever your Tableau server is\n",
    "TABLEAU_USERNAME = \"service_username\"\n",
    "TABLEAU_PASSWORD = \"service_password\"\n",
    "TABLEAU_SITE = \"\"  \n",
    "PROJECT_NAME = \"Game Analytics\"  \n",
    "\n",
    "# Authenticate with Tableau Server\n",
    "auth_url = f\"{TABLEAU_SERVER_URL}/api/3.10/auth/signin\"\n",
    "auth_payload = {\n",
    "    \"credentials\": {\n",
    "        \"name\": TABLEAU_USERNAME,\n",
    "        \"password\": TABLEAU_PASSWORD,\n",
    "        \"site\": {\"contentUrl\": TABLEAU_SITE}\n",
    "    }\n",
    "}\n",
    "auth_response = requests.post(auth_url, json=auth_payload)\n",
    "if auth_response.status_code == 200:\n",
    "    auth_token = auth_response.json()[\"credentials\"][\"token\"]\n",
    "    site_id = auth_response.json()[\"credentials\"][\"site\"][\"id\"]\n",
    "    print(\"Authentication Successful\")\n",
    "else:\n",
    "    print(f\"Authentication Failed: {auth_response.text}\")\n",
    "    exit()\n",
    "\n",
    "# Get Project ID\n",
    "projects_url = f\"{TABLEAU_SERVER_URL}/api/3.10/sites/{site_id}/projects\"\n",
    "headers = {\"X-Tableau-Auth\": auth_token}\n",
    "projects_response = requests.get(projects_url, headers=headers)\n",
    "projects = projects_response.json()[\"projects\"][\"project\"]\n",
    "project_id = next((p[\"id\"] for p in projects if p[\"name\"] == PROJECT_NAME), None)\n",
    "\n",
    "if not project_id:\n",
    "    print(f\" Project '{PROJECT_NAME}' not found!\")\n",
    "    exit()\n",
    "\n",
    "# Upload `.hyper` File\n",
    "upload_url = f\"{TABLEAU_SERVER_URL}/api/3.10/sites/{site_id}/datasources\"\n",
    "files = {\"file\": open(LOCAL_HYPER_FILE, \"rb\")}\n",
    "upload_response = requests.post(upload_url, headers=headers, files=files)\n",
    "\n",
    "if upload_response.status_code == 201:\n",
    "    print(\"`.hyper` file successfully uploaded to Tableau Server\")\n",
    "else:\n",
    "    print(f\"Upload Failed: {upload_response.text}\")\n",
    "\n",
    "# Logout \n",
    "logout_url = f\"{TABLEAU_SERVER_URL}/api/3.10/auth/signout\"\n",
    "requests.post(logout_url, headers=headers)\n",
    "print(\"Logged out of Tableau Server\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
